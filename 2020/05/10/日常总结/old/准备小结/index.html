<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>准备小结 | wang's bolg</title><noscript>开启JavaScript才能访问本站哦~</noscript><link rel="icon" href="/img/pwa/favicon.png"><!-- index.css--><link rel="stylesheet" href="/css/index.css?v=3.0.19"><!-- inject head--><link rel="canonical" href="http://www.wqkenqing.ren/2020/05/10/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93/index.html"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><!-- aplayer--><!-- swiper--><!-- fancybox ui--><!-- katex--><!-- Open Graph--><meta name="description" content="准备小结 hdfs存储机制是怎样的?client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode"><!-- pwa--><meta name="apple-mobile-web-app-capable" content="wang's bolg"><meta name="theme-color" content="var(--efu-main)"><meta name="apple-mobile-web-app-status-bar-style" content="var(--efu-main)"><link rel="bookmark" href="/img/pwa/favicon.png"><link rel="apple-touch-icon" href="/img/pwa/favicon.png" sizes="180x180"><script>console.log(' %c Solitude %c ' + '3.0.19' + ' %c https://github.com/everfu/hexo-theme-solitude',
    'background:#35495e ; padding: 1px; border-radius: 3px 0 0 3px;  color: #fff',
    'background:#ff9a9a ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff',
    'background:unset ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff')
</script><script>(()=>{
        const saveToLocal = {
            set: function setWithExpiry(key, value, ttl) {
                if (ttl === 0)
                    return
                const now = new Date()
                const expiryDay = ttl * 86400000
                const item = {
                    value: value,
                    expiry: now.getTime() + expiryDay
                }
                localStorage.setItem(key, JSON.stringify(item))
            },
            get: function getWithExpiry(key) {
                const itemStr = localStorage.getItem(key)

                if (!itemStr) {
                    return undefined
                }
                const item = JSON.parse(itemStr)
                const now = new Date()

                if (now.getTime() > item.expiry) {
                    localStorage.removeItem(key)
                    return undefined
                }
                return item.value
            }
        };
        window.utils = {
            saveToLocal: saveToLocal,
            getCSS: (url, id = false) => new Promise((resolve, reject) => {
              const link = document.createElement('link')
              link.rel = 'stylesheet'
              link.href = url
              if (id) link.id = id
              link.onerror = reject
              link.onload = link.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                link.onload = link.onreadystatechange = null
                resolve()
              }
              document.head.appendChild(link)
            }),
            getScript: (url, attr = {}) => new Promise((resolve, reject) => {
              const script = document.createElement('script')
              script.src = url
              script.async = true
              script.onerror = reject
              script.onload = script.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                script.onload = script.onreadystatechange = null
                resolve()
              }

              Object.keys(attr).forEach(key => {
                script.setAttribute(key, attr[key])
              })

              document.head.appendChild(script)
            }),
            addGlobalFn: (key, fn, name = false, parent = window) => {
                const globalFn = parent.globalFn || {}
                const keyObj = globalFn[key] || {}

                if (name && keyObj[name]) return

                name = name || Object.keys(keyObj).length
                keyObj[name] = fn
                globalFn[key] = keyObj
                parent.globalFn = globalFn
            },
            addEventListenerPjax: (ele, event, fn, option = false) => {
              ele.addEventListener(event, fn, option)
              utils.addGlobalFn('pjax', () => {
                  ele.removeEventListener(event, fn, option)
              })
            },
            diffDateFormat: (selector) => {
                selector.forEach(item => {
                    const date = new Date(item.getAttribute('datetime') || item.textContent);
                    item.textContent = (date.getMonth() + 1).toString()+'/'+date.getDate().toString();
                });
            },
        }
    })()</script><!-- theme--><script>initTheme = () => {
    let isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const cachedMode = utils.saveToLocal.get('theme');
    if (cachedMode === undefined) {
        const nowMode =
            isDarkMode ? 'dark' : 'light'
        document.documentElement.setAttribute('data-theme', nowMode);
    } else {
        document.documentElement.setAttribute('data-theme', cachedMode);
    }
    typeof rm === 'object' && rm.mode(cachedMode === 'dark' && isDarkMode)
}
initTheme()</script><!-- global head--><script>const GLOBAL_CONFIG = {
    root: '/',
    algolia: undefined,
    localsearch: undefined,
    runtime: '2023-04-20 00:00:00',
    lazyload: {
        enable: false,
        error: '/img/error_load.avif'
    },
    copyright: false,
    highlight: {"limit":200,"expand":true,"copy":true,"syntax":"highlight.js"},
    randomlink: false,
    lang: {"theme":{"dark":"已切换至深色模式","light":"已切换至浅色模式"},"copy":{"success":"复制成功","error":"复制失败"},"backtop":"返回顶部","time":{"day":"天前","hour":"小时前","just":"刚刚","min":"分钟前","month":"个月前"},"day":" 天","f12":"开发者模式已打开，请遵循GPL协议。","totalk":"无需删除空行，直接输入评论即可"},
    aside: {
        state: {
            morning: "✨ 早上好，新的一天开始了",
            noon: "🍲 午餐时间",
            afternoon: "🌞 下午好",
            night: "早点休息",
            goodnight: "晚安 😴",
        },
        witty_words: [],
        witty_comment: {
            prefix: '好久不见，',
            back: '欢迎再次回来，',
        },
    },
    covercolor: {
        enable: false
    },
    comment: false,
    lightbox: 'null',
    right_menu: false,
    translate: {"translateDelay":0,"defaultEncoding":2},
    lure: false,
    expire: false,
};</script><!-- page-config head--><script id="config-diff">var PAGE_CONFIG = {
    is_post: true,
    is_page: false,
    is_home: false,
    page: '',
    toc: true,
    comment: false,
    ai_text: false,
    color: false,
}</script><meta name="generator" content="Hexo 7.3.0"></head><body id="body"><!-- universe--><!-- background img--><!-- loading--><!-- console--><!-- sidebar--><div id="sidebar" style="zoom: 1;"><div id="menu-mask" style="display: none;"></div><div id="sidebar-menus"><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">60</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div></div></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><span class="darkmode_switchbutton menu-child" onclick="sco.switchDarkMode()"><i class="solitude fas fa-circle-half-stroke"></i><span>显示模式</span></span></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="card-tag-cloud"><a href="/tags/linux/">linux<sup>1</sup></a><a href="/tags/flink/">flink<sup>1</sup></a><a href="/tags/kafka/">kafka<sup>2</sup></a><a href="/tags/python/">python<sup>2</sup></a><a href="/tags/elasticsearch-kibana-head-vscode/">elasticsearch kibana head vscode<sup>1</sup></a><a href="/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/">日常总结<sup>4</sup></a><a href="/tags/bigdata/">bigdata<sup>3</sup></a><a href="/tags/%E5%AD%A6%E4%B9%A0spark/">学习spark<sup>1</sup></a><a href="/tags/%E5%AD%A6%E4%B9%A0spark2/">学习spark2<sup>1</sup></a><a href="/tags/spark%E5%AD%A6%E4%B9%A0/">spark学习<sup>1</sup></a><a href="/tags/%E5%B0%8F%E7%BB%93/">小结<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库<sup>1</sup></a><a href="/tags/sparkstream/">sparkstream<sup>1</sup></a><a href="/tags/qa/">qa<sup>1</sup></a><a href="/tags/spark-dependency/">spark dependency<sup>1</sup></a><a href="/tags/cvim/">cvim<sup>1</sup></a><a href="/tags/sparkstreaming/">sparkstreaming<sup>1</sup></a><a href="/tags/es-feild-script/">es feild script<sup>1</sup></a><a href="/tags/tmux/">tmux<sup>1</sup></a><a href="/tags/%E8%BF%90%E7%BB%B4-cdn-%E5%8A%A0%E9%80%9F/">运维 cdn 加速<sup>1</sup></a><a href="/tags/elasticsearch-drawio/">elasticsearch drawio<sup>1</sup></a><a href="/tags/python-learn/">python learn<sup>1</sup></a><a href="/tags/%E5%B8%B8%E8%A7%84api%E5%B0%81%E8%A3%85/">常规api封装<sup>1</sup></a><a href="/tags/DDNS/">DDNS<sup>1</sup></a></div></div></div></div><!-- keyboard--><!-- righhtside--><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav class="show" id="nav"><div id="nav-group"><div id="blog_name"><a id="site-name" href="/" title="返回博客主页"><span class="title">Solitude</span><i class="solitude fas fa-home"></i></a></div><div id="page-name-mask"><div id="page-name"><a id="page-name-text" onclick="sco.toTop()">准备小结</a></div></div><div id="menus"></div><div id="nav-left"></div><div id="nav-right"><div class="nav-button" id="nav-totop" onclick="sco.toTop()"><a class="totopbtn"><i class="solitude fas fa-arrow-up"></i><span id="percent">0</span></a></div><div id="toggle-menu"><a class="site-page"><i class="solitude fas fa-bars"></i></a></div></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original" title="该文章为原创文章，注意版权协议">原创</a><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%B0%8F%E7%BB%93/"><span class="tags-name tags-punctuation"><i class="solitude fas fa-hashtag"></i>小结</span></a></div></div></div></div><h1 class="post-title">准备小结</h1><div id="post-meta"><div class="meta-secondline"></div></div></div><article class="post-content article-container"><h1 id="准备小结"><a href="#准备小结" class="headerlink" title="准备小结"></a>准备小结</h1><span id="more"></span>
<h2 id="hdfs存储机制是怎样的"><a href="#hdfs存储机制是怎样的" class="headerlink" title="hdfs存储机制是怎样的?"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>
<h2 id="hadoop中combiner的作用是什么"><a href="#hadoop中combiner的作用是什么" class="headerlink" title="hadoop中combiner的作用是什么?"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p>
<h2 id="你们数据库怎么导入hive-的-有没有出现问题"><a href="#你们数据库怎么导入hive-的-有没有出现问题" class="headerlink" title="你们数据库怎么导入hive 的,有没有出现问题"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p>
<h2 id="hdfs-site-xml的3个主要属性"><a href="#hdfs-site-xml的3个主要属性" class="headerlink" title="hdfs-site.xml的3个主要属性?"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p>
<h2 id="下列哪项通常是集群的最主要瓶颈"><a href="#下列哪项通常是集群的最主要瓶颈" class="headerlink" title="下列哪项通常是集群的最主要瓶颈"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p>
<h2 id="关于-SecondaryNameNode-哪项是正确的？"><a href="#关于-SecondaryNameNode-哪项是正确的？" class="headerlink" title="关于 SecondaryNameNode 哪项是正确的？"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p>
<h2 id="mapreduce的原理"><a href="#mapreduce的原理" class="headerlink" title="mapreduce的原理?"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p>
<h2 id="HDFS存储的机制"><a href="#HDFS存储的机制" class="headerlink" title="HDFS存储的机制?"></a>HDFS存储的机制?</h2><h3 id="写流程："><a href="#写流程：" class="headerlink" title="写流程："></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p>
<h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><h2 id="举一个简单的例子说明mapreduce是怎么来运行的"><a href="#举一个简单的例子说明mapreduce是怎么来运行的" class="headerlink" title="举一个简单的例子说明mapreduce是怎么来运行的 ?"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p>
<p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p>
<p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p>
<p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p>
<h2 id="了解hashMap-和hashTable吗介绍下，他们有什么区别。"><a href="#了解hashMap-和hashTable吗介绍下，他们有什么区别。" class="headerlink" title="了解hashMap 和hashTable吗介绍下，他们有什么区别。"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id="为什么重写equals还要重写hashcode"><a href="#为什么重写equals还要重写hashcode" class="headerlink" title="为什么重写equals还要重写hashcode"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p>
<h2 id="说一下map的分类和常见的情况"><a href="#说一下map的分类和常见的情况" class="headerlink" title="说一下map的分类和常见的情况"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p>
<ul>
<li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复</li>
</ul>
<h3 id="Hashmap"><a href="#Hashmap" class="headerlink" title="Hashmap"></a>Hashmap</h3><p>是一个最常用的Map</p>
<ul>
<li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li>
<li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li>
<li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li>
<li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap</li>
</ul>
<h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3><p>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</p>
<ul>
<li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢</li>
</ul>
<h3 id="LinkedHashMap"><a href="#LinkedHashMap" class="headerlink" title="LinkedHashMap"></a>LinkedHashMap</h3><p>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关</p>
<h3 id="TreeMap"><a href="#TreeMap" class="headerlink" title="TreeMap"></a>TreeMap</h3><p>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</p>
<p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p>
<hr>
<h2 id="Object若不重写hashCode-的话，hashCode-如何计算出来的？"><a href="#Object若不重写hashCode-的话，hashCode-如何计算出来的？" class="headerlink" title="Object若不重写hashCode()的话，hashCode()如何计算出来的？"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p>
<h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><h3 id="1-spark的有几种部署模式，每种模式特点？"><a href="#1-spark的有几种部署模式，每种模式特点？" class="headerlink" title="1. spark的有几种部署模式，每种模式特点？"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>本地模式分三类</p>
<ul>
<li>local：只启动一个executor</li>
<li>local[k]: 启动k个executor</li>
<li>local[*]：启动跟cpu数目相同的 executor</li>
</ul>
<h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p>
<h4 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p>
<h4 id="Spark-on-yarn模式"><a href="#Spark-on-yarn模式" class="headerlink" title="Spark on yarn模式"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p>
<h3 id="2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id="Spark-core"><a href="#Spark-core" class="headerlink" title="Spark core"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p>
<h4 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p>
<h4 id="Spark-sql："><a href="#Spark-sql：" class="headerlink" title="Spark sql："></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p>
<h4 id="MLBase"><a href="#MLBase" class="headerlink" title="MLBase"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>
<h4 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p>
<h4 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p>
<ul>
<li><a href="https://blog.csdn.net/yirenboy/article/details/47441465">https://blog.csdn.net/yirenboy/article/details/47441465</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author_group"><div class="post-copyright__author_img"><img class="post-copyright__author_img_front" src="/img/logo.png"></div><div class="post-copyright__author_name">Kuiq  Wang</div><div class="post-copyright__author_desc"></div></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div id="quit-box" onclick="RemoveRewardMask()"></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本文是原创文章，采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans">CC BY-NC-SA 4.0</a>协议，完整转载请注明来自<a href="/">wang's bolg</a></span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%B0%8F%E7%BB%93/"><span class="tags-punctuation"><i class="solitude fas fa-hashtag"></i>小结<span class="tagsPageCount">1</span></span></a></div></div></div><nav class="needEndHide pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">日常运维</div></div></a></div><div class="next-post pull-right"><a href="/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">es常用命令</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><div class="top-group"><div class="sayhi" id="sayhi" onclick="sco.changeWittyWord()"></div></div></div><div class="avatar"><img alt="头像" src="/img/logo.png"></div><div class="description"></div><div class="bottom-group"><span class="left"><div class="name">Kuiq  Wang</div><div class="desc">只有迎风，风筝才能飞得更高。</div></span><div class="social-icons is-center"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="solitude fas fa-bars"></i><span>文章目录</span></div><div class="toc-content" id="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93"><span class="toc-text">准备小结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84"><span class="toc-text">hdfs存储机制是怎样的?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop%E4%B8%ADcombiner%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">hadoop中combiner的作用是什么?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%A0%E4%BB%AC%E6%95%B0%E6%8D%AE%E5%BA%93%E6%80%8E%E4%B9%88%E5%AF%BC%E5%85%A5hive-%E7%9A%84-%E6%9C%89%E6%B2%A1%E6%9C%89%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98"><span class="toc-text">你们数据库怎么导入hive 的,有没有出现问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs-site-xml%E7%9A%843%E4%B8%AA%E4%B8%BB%E8%A6%81%E5%B1%9E%E6%80%A7"><span class="toc-text">hdfs-site.xml的3个主要属性?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E5%88%97%E5%93%AA%E9%A1%B9%E9%80%9A%E5%B8%B8%E6%98%AF%E9%9B%86%E7%BE%A4%E7%9A%84%E6%9C%80%E4%B8%BB%E8%A6%81%E7%93%B6%E9%A2%88"><span class="toc-text">下列哪项通常是集群的最主要瓶颈</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-SecondaryNameNode-%E5%93%AA%E9%A1%B9%E6%98%AF%E6%AD%A3%E7%A1%AE%E7%9A%84%EF%BC%9F"><span class="toc-text">关于 SecondaryNameNode 哪项是正确的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mapreduce%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">mapreduce的原理?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="toc-text">HDFS存储的机制?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-text">写流程：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="toc-text">读流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BE%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90%E8%AF%B4%E6%98%8Emapreduce%E6%98%AF%E6%80%8E%E4%B9%88%E6%9D%A5%E8%BF%90%E8%A1%8C%E7%9A%84"><span class="toc-text">举一个简单的例子说明mapreduce是怎么来运行的 ?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%86%E8%A7%A3hashMap-%E5%92%8ChashTable%E5%90%97%E4%BB%8B%E7%BB%8D%E4%B8%8B%EF%BC%8C%E4%BB%96%E4%BB%AC%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E3%80%82"><span class="toc-text">了解hashMap 和hashTable吗介绍下，他们有什么区别。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E5%86%99equals%E8%BF%98%E8%A6%81%E9%87%8D%E5%86%99hashcode"><span class="toc-text">为什么重写equals还要重写hashcode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%B4%E4%B8%80%E4%B8%8Bmap%E7%9A%84%E5%88%86%E7%B1%BB%E5%92%8C%E5%B8%B8%E8%A7%81%E7%9A%84%E6%83%85%E5%86%B5"><span class="toc-text">说一下map的分类和常见的情况</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hashmap"><span class="toc-text">Hashmap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hashtable"><span class="toc-text">Hashtable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LinkedHashMap"><span class="toc-text">LinkedHashMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TreeMap"><span class="toc-text">TreeMap</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Object%E8%8B%A5%E4%B8%8D%E9%87%8D%E5%86%99hashCode-%E7%9A%84%E8%AF%9D%EF%BC%8ChashCode-%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="toc-text">Object若不重写hashCode()的话，hashCode()如何计算出来的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark"><span class="toc-text">spark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-spark%E7%9A%84%E6%9C%89%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%EF%BC%8C%E6%AF%8F%E7%A7%8D%E6%A8%A1%E5%BC%8F%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-text">1. spark的有几种部署模式，每种模式特点？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F"><span class="toc-text">本地模式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cluster%E6%A8%A1%E5%BC%8F"><span class="toc-text">cluster模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#standalone%E6%A8%A1%E5%BC%8F"><span class="toc-text">standalone模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-on-yarn%E6%A8%A1%E5%BC%8F"><span class="toc-text">Spark on yarn模式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Spark%E6%8A%80%E6%9C%AF%E6%A0%88%E6%9C%89%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%BB%84%E4%BB%B6%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E5%8A%9F%E8%83%BD%EF%BC%8C%E9%80%82%E5%90%88%E4%BB%80%E4%B9%88%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9F"><span class="toc-text">2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-core"><span class="toc-text">Spark core</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkStreaming"><span class="toc-text">SparkStreaming</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-sql%EF%BC%9A"><span class="toc-text">Spark sql：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MLBase"><span class="toc-text">MLBase</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GraphX"><span class="toc-text">GraphX</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark%E6%9C%89%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6"><span class="toc-text">spark有哪些组件</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="solitude fas fa-map"></i><span>最近发布</span></div><div class="aside-list"><a class="aside-list-item" href="/2020/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E5%88%86%E4%BA%AB/%E6%95%B0%E4%BB%93%E5%88%86%E4%BA%AB/" title="数仓分享"><div class="content"><span class="title" href="/2020/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E5%88%86%E4%BA%AB/%E6%95%B0%E4%BB%93%E5%88%86%E4%BA%AB/" title="数仓分享">数仓分享</span></div></a><a class="aside-list-item" href="/2020/05/26/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/pythonQA/" title="pythonQA"><div class="content"><span class="title" href="/2020/05/26/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/pythonQA/" title="pythonQA">pythonQA</span></div></a><a class="aside-list-item" href="/2020/05/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/python/" title="python 小结2"><div class="content"><span class="title" href="/2020/05/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/python/" title="python 小结2">python 小结2</span></div></a><a class="aside-list-item" href="/2020/05/22/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/es%E6%B7%BB%E5%8A%A0%E5%AD%97%E6%AE%B5%E5%92%8C%E9%BB%98%E8%AE%A4%E5%80%BC/" title="es添加字段并赋值"><div class="content"><span class="title" href="/2020/05/22/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/es%E6%B7%BB%E5%8A%A0%E5%AD%97%E6%AE%B5%E5%92%8C%E9%BB%98%E8%AE%A4%E5%80%BC/" title="es添加字段并赋值">es添加字段并赋值</span></div></a><a class="aside-list-item" href="/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7/" title="es常用操作工具"><div class="content"><span class="title" href="/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7/" title="es常用操作工具">es常用操作工具</span></div></a></div></div></div></div></main><footer id="footer"><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div class="copyright">© 2023 - 2025 By&nbsp;<a class="footer-bar-link" href="/"><img class="author-avatar" src="/img/pwa/favicon.png">Kuiq  Wang</a></div><div class="beian-group"><a class="footer-bar-link" href="https://hexo.io/">框架：Hexo</a><a class="footer-bar-link" href="https://github.com/everfu/hexo-theme-solitude">主题：Solitude</a></div></div></div></div></footer></div><!-- right_menu--><!-- inject body--><div><script src="/js/utils.js?v=3.0.19"></script><script src="/js/main.js?v=3.0.19"></script><script src="/js/third_party/waterfall.min.js?v=3.0.19"></script><script src="https://fastly.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/tw_cn.js?v=3.0.19"></script><script src="https://fastly.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>window.paceOptions = {
  restartOnPushState: false
}

utils.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')
</script><script src="https://fastly.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div class="js-pjax"></div></div><!-- pjax--><script>const pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: ["title","#body-wrap","#site-config","meta[name=\"description\"]",".js-pjax","meta[property^=\"og:\"]","#config-diff",".rs_show",".rs_hide"],
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
})

document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
})

document.addEventListener('pjax:complete', () => {
    window.refreshFn()

    document.querySelectorAll('script[data-pjax]').forEach(item => {
        const newScript = document.createElement('script')
        const content = item.text || item.textContent || item.innerHTML || ""
        Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
        newScript.appendChild(document.createTextNode(content))
        item.parentNode.replaceChild(newScript, item)
    })

    GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

})

document.addEventListener('pjax:error', (e) => {
    if (e.request.status === 404) {
        pjax.loadUrl('/404.html')
    }
})</script><!-- google adsense--><!-- search--><!-- music--></body></html>
        <script>
            const posts = ["2020/07/16/日常总结/分享/数仓分享/","2020/05/26/日常总结/编程/pythonQA/","2020/05/25/日常总结/编程/python/","2020/05/22/日常总结/运维/es添加字段和默认值/","2020/05/21/日常总结/es/es常用操作工具/","2020/05/21/日常总结/运维/vscode插件/","2020/05/20/日常总结/运维/阿里云DDNS/","2020/05/19/日常总结/运维/cvim f失灵问题修复/","2020/05/19/日常总结/运维/tmux上手/","2020/05/11/日常总结/kafka/kafka实现/","2020/05/11/日常总结/old/Lamda积累/","2020/05/11/日常总结/old/kafka/","2020/05/11/日常总结/old/mapreduce组件总结/","2020/05/11/日常总结/old/大数据相关分享/","2020/05/11/日常总结/spark/spark常见的输入源/","2020/05/11/日常总结/运维/日常运维/","2020/05/10/日常总结/old/准备小结/","2020/02/25/日常总结/es/es常用命令/","2019/12/24/日常总结/old/hive2总结/","2019/07/31/日常总结/flink/Flink/","2019/07/16/oldblog/List学习/","2019/07/16/oldblog/Restful架构/","2019/07/16/oldblog/MapReduce的核心思想/","2019/07/16/oldblog/Hadoop总结/","2019/07/16/oldblog/String对比/","2019/07/16/oldblog/Vim笔记/","2019/07/16/oldblog/hbase/","2019/07/16/oldblog/flume小结/","2019/07/16/oldblog/JVM问题/","2019/07/16/oldblog/git小结/","2019/07/16/oldblog/java---GC机制/","2019/07/16/oldblog/java中IO小结/","2019/07/16/oldblog/java中枚举的使用/","2019/07/16/oldblog/java中的形参与实参的理解/","2019/07/16/oldblog/java八大数据类型总结/","2019/07/16/oldblog/kafka小结/","2019/07/16/oldblog/map总结/","2019/07/16/oldblog/maven小结/","2019/07/16/oldblog/异常处理机制小结/","2019/07/16/oldblog/反射小结/","2019/07/16/oldblog/文本处理小结/","2019/07/16/oldblog/测试心得/","2019/07/16/oldblog/sed & awk小结/","2019/07/16/oldblog/爬虫之nutch/","2019/07/16/oldblog/python learn/","2019/07/16/日常总结/oldblog/blog17/","2019/07/16/日常总结/spark/宽窄依赖/","2019/07/16/日常总结/spark/stream2/","2019/07/15/日常总结/spark/stream/","2019/06/13/日常总结/old/Yarn配置/","2019/06/13/日常总结/old/flume记录/","2019/05/17/日常总结/spark/spark操作/","2019/05/17/日常总结/hadoop/hbase/hbase操作/","2019/05/16/日常总结/hadoop/hadoopHA搭建/","2018/12/24/日常总结/old/hive总结/","2018/06/04/日常总结/old/hbase积累/","2018/03/04/日常总结/old/spark学习/","2018/03/04/日常总结/old/spark学习2/","2018/03/04/日常总结/old/spark算子/","2018/03/04/日常总结/old/sqoop记录/"];
            function toRandomPost() {
                const randomPost = posts[Math.floor(Math.random() * posts.length)];
                pjax.loadUrl(GLOBAL_CONFIG.root + randomPost);
            }
        </script>